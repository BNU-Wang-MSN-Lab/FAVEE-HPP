# Across_history

## 1.GPT4_DESC

We employed the state-of-the-art LLM, GPT-4.0 to generate relationship-specific descriptions, establishing a contextual framework for the subsequent representations of relationships by the language model. Specially, the temperature parameter for GPT-4.0 was set to  zero to ensure reproducibility.

For the modern NLP experiment, we adopted 258 Chinese NLP relationships from Study2. For the experiment of ancient Chinese, a group of experts in ancient Chinese language, literature and history were invited to participate in this study. They selected 120 typical relationships that existed in ancient Chinese society. After obtaining the relationship labels and descriptions from GPT-4.0, they manually checked and modified the labels and descriptions generated by LLM to better reflect the linguistic features and relationship characteristics of the ancient era.

## 2.BERT_embedding

We adopted a novel approach to generate human-like PLM (pretrained language model), which was previously proposed by Culter & Condon (2022) to identify Big Five personality structures in language model.

For modern Chinese PLM model, we employed the word-based Chinese-RoBERTa-Baed model from UER-py Modelzoo (Zhao et al., 2019). For ancient Chinese PLM model, we used the BERT ancient-Chinese model (Wang & Ren, 2022). The two models are stored in OSF (https://osf.io/nfkmj), but you can also download them from Huggingface.

- modern model: https://huggingface.co/uer/roberta-base-word-chinese-cluecorpussmall

- ancient model: https://huggingface.co/Jihuai/bert-ancient-chinese

The BERT embeddings have been stored on OSF (file path: NLP/bert_embedding_data.zip) for accessibility. 

## 3.Human_compare&Model_compare

- 1.transform_format.ipynb + 2.caculate_similarity_matrix.ipynb

  Transform PLM embeddings (modern: 258×768; ancient: 120×768) into a cosine similarity matrix (modern: 258×258; ancient: 120×120).

- 3.correlation_human_bert.ipynb

  To retrieve the most human-like relationship representations, we tested different query types and layers. Each query type could yield multiple query texts, resulting in seven texts for modern Chinese and ten texts for ancient Chinese.

- 4.PCA.ipynb

  PCA on modern Chinese PLM representations generated components (V1-V4, V6) corresponding well with FAVEE structures (*rs* > 0.47). FAVEE structures can also be identified after applying PCA on ancient PLM embeddings.

- 5.1RSA_correlation.ipynb

  For both FAVEE dimensions and HPP categories, we found significant correlations between RDMs in human ratings and RDMs in ancient PLM embeddings.

- 5.2Model_compare.ipynb

  Model comparison analysis suggested that the FAVEE model outperformed other theoretical models in terms of the RSA correlation with human ratings, highlighting its stable superiority over other relationship models across times.

- 6.culture_evolution.ipynb

  To further reveal the difference between ancient and modern China, we evaluated the relative contribution of each FAVEE dimension when predicting relationship representations in ancient and modern PLM. We found that “formality” explained larger variance in modern than ancient times, whereas “equality” accounted for larger variance in ancient than modern times. This suggested that, comparing with modern Chinese, ancient Chinese might weight more on equality features (e.g., social hierarchy) when understanding relationships but concern less on formality features (e.g., occupations).

# Expert_reliability

To examine whether ancient Chinese PLM had expertise like human experts, we recruited a group of scholars (n=44) specializing in ancient Chinese cultures and compared their estimation of ancient Chinese’s relationship understanding with PLM representations.

They were instructed to take the perspective of ancient Chinese and rated 120 relationships across 33 dimensional features in the context of ancient China.

RSA correlation revealed that ancient Chinese PLM embeddings exhibited higher agreement with ancient Chinese experts than non-experts. Additionally, ancient Chinese PLM embeddings also displayed a country-specific (or language-specific) advantages, with higher agreement with modern Chinese than modern Americans.

- remove_name: Raw data collected from ancient experts, with names removed for privacy.

- output_data/clean_results/...: Averaged matrix representing ancient relationships across 33-dimensional features.

  We originally collected 130 ancient relationships. After removing 10 duplicate relationships, where more than one ancient relationship corresponded to the same modern relationship, we were left with 120 relationships.  

  - Files named ending with "_chi" correspond to labels aligned with modern labels.

